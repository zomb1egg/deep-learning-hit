{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f19443-dcc5-483e-ad2c-800081f5db16",
   "metadata": {},
   "source": [
    "# Homework #3: Proof that ReLU follows Universal Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eab20a",
   "metadata": {},
   "source": [
    "### The Universal Approximation Theorem is defined as follows: \n",
    "$\\\\[5mm]$\n",
    "$\n",
    "\\text{Let } f : [a, b] \\to \\mathbb{R} \\text{ be a continuous function, where } [a, b] \\subset \\mathbb{R}. \\text{ For any } \\epsilon > 0, \\\\\n",
    "\\text{there exists a feedforward neural network with a single hidden layer such that} \\\\\n",
    "\\sup_{x \\in [a, b]} \\left| f(x) - \\hat{f}(x) \\right| < \\epsilon,\n",
    "\\text{ where } \\hat{f}(x) \\text{ is the output of the neural network.}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59eb3f",
   "metadata": {},
   "source": [
    "### steps to prove that UAT applies on ReLU\n",
    "1) prove that ReLU can represent a sigmoid-like function $\\tilde{\\sigma}(x)$\n",
    "2) show that $\\tilde{\\sigma}(x)$ can in turn approximate the indicator function $\\chi_{[x_0, x_1]}(x)$ \n",
    "3) via the Stone-Weierstrass Theorem, our neural network can be represented as the sum of $\\tilde{\\sigma}(x)$\n",
    "4) find the Error Bound of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2b68b",
   "metadata": {},
   "source": [
    "### a couple of important definitions before we begin:\n",
    "\n",
    "$\n",
    "\\text{ReLU}(x) = \\max{\\{0, x\\}} = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x \\geq 0, \\\\\n",
    "0 & \\text{if } x < 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "<div style=\"margin-bottom: -10px; font-size: 20px\">Stone-Weierstrass theorem: </div> \n",
    "\n",
    "it states that if $f(x)$ is a continuous function defined on a compact interval $[a, b]$, then,  \\\n",
    "$f(x)$ can be uniformly approximated by a finite linear combination of basis functions: \\\n",
    "$\\displaystyle f(x) \\approx \\sum_{i = 1}^N c_i\\phi_i(x)$ \\\n",
    "where $\\phi_i(x)$ are continuous functions and $ci \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010822cc",
   "metadata": {},
   "source": [
    "## Step 1. Sigmoidal representation of ReLU\n",
    "For this, we will firstly construct a bound sigmoid-like function that will look something like this:\n",
    "\n",
    "$\n",
    "\\tilde{f}(x) = \\begin{cases} \n",
    "0 & \\text{if } x < 0, \\\\\n",
    "x & \\text{if } 0 \\leq x \\leq 1, \\\\\n",
    "1 & \\text{if } x > 1.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "By subtracting 2 ReLU functions we get that any function of type $\\tilde{f}(wx + b)$ can be represented as such:\n",
    "\n",
    "$$\n",
    "\\tilde{f}(wx + b) = \\text{ReLU}(wx + b) - \\text{ReLU}(wx + (b - 1))\n",
    "$$ \\\n",
    "why is that? \\\n",
    "lets begin by simplifying this equation: \\\n",
    "\\\n",
    "$\n",
    "\\displaystyle\n",
    "\\text{ReLU}(wx + b) - \\text{ReLU}(wx + (b - 1)) = \\max{\\{0, wx + b\\}} - \\max{\\{0, wx + (b - 1)\\}} = \n",
    "$ \\\n",
    "$\n",
    "\\displaystyle\n",
    "=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "wx + b & \\text{if } x \\geq -\\frac{b}{w} \\\\\n",
    "0 & \\text{if } x < -\\frac{b}{w} \\\\\n",
    "\\end{array}\n",
    "\\right\\} - \\left\\{\n",
    "\\begin{array}{ll}\n",
    "wx + (b - 1) & \\text{if } x \\geq \\frac{(1 - b)}{w} \\\\\n",
    "0 & \\text{if } x < \\frac{(1 - b)}{w}  \\\\\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "$ \n",
    "\n",
    "${\\color{red}*}\\ \n",
    "wx + b \\geq 0 \\implies x \\geq -\\frac{b}{w}\n",
    "$ \\\n",
    "${\\color{red}*}\\ \n",
    "wx + (b - 1) \\geq 0 \\implies x \\geq \\frac{(1 - b)}{w}\n",
    "$ \\\n",
    "we know that $-\\frac{b}{w} > \\frac{(1-b)}{w}$, so: \\\n",
    "${\\color{red}*}\\\n",
    "x > \\frac{(1 - b)}{w} \\implies wx + b - (wx + (b - 1)) = 1 \n",
    "$ \\\n",
    "${\\color{red}*}\\\n",
    "-\\frac{b}{w} \\leq x \\leq \\frac{(1 - b)}{w} \\implies wx + b - 0 = wx + b\n",
    "$ \\\n",
    "${\\color{red}*}\\\n",
    "x < -\\frac{b}{w} \\implies 0 - 0 = 0\n",
    "$ \n",
    "\n",
    "and so we get:\n",
    "\n",
    "$\n",
    "\\displaystyle \n",
    "\\tilde{f}(wx + b) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if } x > \\frac{(1 - b)}{w}, \\\\\n",
    "wx + b & \\text{if } -\\frac{b}{w} \\leq x \\leq \\frac{(1 - b)}{w}, \\\\\n",
    "0 & \\text{if } x < -\\frac{(b)}{w} , \\\\\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "from that we got a function that \"mimics\" our known sigmoid function.\n",
    "\n",
    "$\n",
    "\\displaystyle\n",
    "\\lim_{x\\to\\infin}(\\tilde{f}(wx + b)) = 1 \\ \\ \\ \n",
    "\\lim_{x\\to-\\infin}(\\tilde{f}(wx + b)) = 0 \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0728f4",
   "metadata": {},
   "source": [
    "## Step 2. approximation of indicator function: \n",
    "from the published proof we know that a sigmoidal function can estimate the step indicator function: \\\n",
    "$\n",
    "\\chi_{[x_0, x_1]}(x) =  \\begin{cases} \n",
    "x & \\text{if } x \\in [x_0, x_1] \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$\\\n",
    "we also know that for large values of $w$, the function $\\tilde{f}(wx + b)$ transitions sharply at $\\displaystyle x = -\\frac{b}{w}$. as such: \\\n",
    "$\n",
    "\\tilde{f}(wx + b) = \\begin{cases} \n",
    "1 & \\text{if } x > -\\frac{b}{w} \\\\\n",
    "0 & \\text{if } x < -\\frac{b}{w}  \\\\\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b4bc3",
   "metadata": {},
   "source": [
    "## Step 3. Construction of the neural network\n",
    "we know that ReLU is a continues function, it is a combination of a constant (0) and a continues function ($x$), and their merging point is:\n",
    "\n",
    "$\n",
    "\\displaystyle\n",
    "\\lim_{x \\to 0^+} ReLU(x) = \\lim_{x \\to 0^+} x = 0 \n",
    "$  \n",
    "<!--ah yes, here we go again with this github KaTeX rendering -->\n",
    "$\n",
    "\\displaystyle\n",
    "\\lim_{x \\to 0^-} ReLU(x) = \\lim_{x \\to 0^-} 0 = 0 \n",
    "$ \n",
    "\n",
    "therefore ReLU is continues and every linear combination of it is continues as well. \\\n",
    "as such we can invoke the Stone-Weierstrass Theorem, \\\n",
    "and define our neural network as a finite sum of sigmoidal functions. \\\n",
    "so, we approximate $f(x)$ as: \\\n",
    "$\n",
    "\\displaystyle\n",
    "\\hat{f}(x) = \\sum_{i = 1}^N c_i\\tilde{f}(w_ix + b_i)\n",
    "$ \\\n",
    "where $w_i$, $b_i$, and $c_i$ are learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef5462",
   "metadata": {},
   "source": [
    "## Step 4. Error Bound\n",
    "We define the approximation error as: \\\n",
    "$E(x) = |f (x) − \\hat{f}(x)|$. \\\n",
    "By the uniform continuity of $f(x)$ (by the Stone-Weierstrass Theorem) and the compactness of $[a, b]$, for any $\\epsilon$ > 0, \\\n",
    "there exist parameters $w_i$, $b_i$, and $c_i$ such that: \\\n",
    "$\n",
    "\\displaystyle\n",
    "\\sup_{x∈[a,b]} E(x) < \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74978a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Thus, any continuous function on a compact interval $[a, b]$ can be approximated \\\n",
    "arbitrarily closely by a single-layer neural network with a sigmoidal activation\n",
    "function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
